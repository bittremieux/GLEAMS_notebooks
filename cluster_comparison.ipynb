{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['GLEAMS_HOME'] = os.path.join(os.environ['HOME'],\n",
    "                                         'Projects/gleams')\n",
    "# Make sure all code is in the PATH.\n",
    "sys.path.append(\n",
    "    os.path.normpath(os.path.join(os.environ['GLEAMS_HOME'], 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import EfficiencyWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=EfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyteomics\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from evaluate_clusters import evaluate_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "from gleams import logger as glogger\n",
    "glogger.init()\n",
    "# Initialize all random seeds before importing any packages.\n",
    "from gleams import rndm\n",
    "rndm.set_seeds()\n",
    "\n",
    "from gleams import config\n",
    "from gleams.cluster import cluster\n",
    "from gleams.feature import spectrum\n",
    "from gleams.ms_io import ms_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('gleams')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette(['#9e0059', '#6da7de', '#ee266d', '#dee000', '#eb861e'])\n",
    "sns.set_context('paper', font_scale=1.3)    # Single-column figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $GLEAMS_HOME/notebooks/cluster_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'notebooks', 'cluster_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_export_spectra_to_mgf(peak_filename, out_filename, scans):\n",
    "    logger.debug('Process file %s', peak_filename)\n",
    "    if not os.path.isfile(peak_filename):\n",
    "        return None\n",
    "    # Read all spectra from the original peak file.\n",
    "    spectrum_idx, spectra_dicts = [], []\n",
    "    for spec in ms_io.get_spectra(peak_filename, scans):\n",
    "        if (config.charges[0] <= spec.precursor_charge <= config.charges[1]\n",
    "                and spectrum.preprocess(copy.deepcopy(spec),\n",
    "                                        config.fragment_mz_min,\n",
    "                                        config.fragment_mz_max).is_valid):\n",
    "            spectra_dicts.append({\n",
    "                'm/z array': spec.mz,\n",
    "                'intensity array': spec.intensity,\n",
    "                'params': {\n",
    "                    'TITLE': f'{os.path.basename(peak_filename)}:scan:{spec.identifier}',\n",
    "                    'RTINSECONDS': spec.retention_time,\n",
    "                    'PEPMASS': spec.precursor_mz,\n",
    "                    'CHARGE': f'{spec.precursor_charge}+'}})\n",
    "            spectrum_idx.append((int(spec.identifier),\n",
    "                                 spec.precursor_charge,\n",
    "                                 spec.precursor_mz))\n",
    "    # Export the spectra to a temporary MGF file.\n",
    "    if len(spectra_dicts) > 0:\n",
    "        pyteomics.mgf.write(spectra_dicts, out_filename, use_numpy=True)\n",
    "    return spectrum_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "max_spectra_per_split = 30_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_parquet(\n",
    "    os.path.join(os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "                 f'embed_{config.massivekb_task_id}_{split}.parquet'))\n",
    "datasets_splits = np.array_split(\n",
    "    datasets.sample(frac=1),\n",
    "    max(3, math.ceil(len(datasets) / max_spectra_per_split)))\n",
    "\n",
    "psms = pd.read_parquet(\n",
    "    os.path.join(os.environ['GLEAMS_HOME'], 'data', 'metadata',\n",
    "                 f'massivekb_ids_{config.massivekb_task_id}.parquet'),\n",
    "    columns=['dataset', 'filename', 'scan', 'sequence'])\n",
    "    \n",
    "for split_i, datasets_split in enumerate(datasets_splits):\n",
    "    cluster_dir_split = os.path.join(cluster_dir, str(split_i))\n",
    "    if not os.path.isdir(cluster_dir_split):\n",
    "        os.makedirs(cluster_dir_split)\n",
    "    cluster_dir_split_tmp = os.path.join(cluster_dir_split, 'tmp')\n",
    "    if os.path.isdir(cluster_dir_split_tmp):\n",
    "        shutil.rmtree(cluster_dir_split_tmp)\n",
    "    os.makedirs(cluster_dir_split_tmp)\n",
    "    filename_md = os.path.join(cluster_dir_split, 'cluster_comparison.parquet')\n",
    "    filename_mgf = os.path.join(cluster_dir_split, 'cluster_comparison.mgf')\n",
    "    if os.path.isfile(filename_md):\n",
    "        continue\n",
    "    if os.path.isfile(filename_mgf):\n",
    "        os.remove(filename_mgf)\n",
    "    logger.info('Partition %d/%d: Export spectra to be clustered to MGF file',\n",
    "                split_i + 1, len(datasets_splits))\n",
    "    spectrum_idx, tmp_filenames = [], []\n",
    "    dataset_filename_scans = (datasets_split.groupby(['dataset', 'filename'])\n",
    "                              ['scan'].apply(list).reset_index())\n",
    "    # FIXME: Replace by `/tmp` if using local copies of the peak files.\n",
    "    for dataset, filename, scans in zip(dataset_filename_scans['dataset'],\n",
    "                                        dataset_filename_scans['filename'],\n",
    "                                        dataset_filename_scans['scan']):\n",
    "        file_spectrum_idx = read_export_spectra_to_mgf(\n",
    "            os.path.join(os.environ['GLEAMS_HOME'], 'data', 'peak', dataset, filename),\n",
    "            os.path.join(cluster_dir_split_tmp, f'{os.path.splitext(filename)[0]}.mgf'),\n",
    "            scans)\n",
    "        if file_spectrum_idx is not None:\n",
    "            for idx in file_spectrum_idx:\n",
    "                spectrum_idx.append((dataset, filename, *idx))\n",
    "            tmp_filenames.append(os.path.join(\n",
    "                cluster_dir_split_tmp, f'{os.path.splitext(filename)[0]}.mgf'))\n",
    "    logger.debug('Combine %d temporary files into a single MGF file %s',\n",
    "                 len(tmp_filenames), filename_mgf)\n",
    "    with open(filename_mgf, 'wb') as f_out:\n",
    "        for fn in tmp_filenames:\n",
    "            with open(fn, 'rb') as f_in:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    shutil.rmtree(cluster_dir_split_tmp)\n",
    "    logger.debug('Export metadata for %d spectra to Parquet file %s',\n",
    "                 len(spectrum_idx), filename_md)\n",
    "    metadata = pd.merge(\n",
    "        pd.DataFrame(spectrum_idx, columns=['dataset', 'filename', 'scan',\n",
    "                                            'charge', 'mz']),\n",
    "        psms, 'left', ['dataset', 'filename', 'scan'])\n",
    "    metadata['sequence'] = metadata['sequence'].str.replace('I', 'L')\n",
    "    metadata.to_parquet(filename_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_sizes = [(5, None)] #, (2, None), (10, None), (50, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_falcon(filename, ids=None):\n",
    "    cluster_labels = pd.read_csv(filename, comment='#')\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = cluster_labels.set_index(\n",
    "            cluster_labels['identifier'].str.rsplit(':', 1)\n",
    "            .str[1].astype(int))\n",
    "        cluster_labels = pd.merge(cluster_labels['cluster'], ids,\n",
    "                                  'left', left_index=True, right_index=True)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_maracluster(filename, ids=None):\n",
    "    cluster_labels = (pd.read_csv(filename, sep='\\t',\n",
    "                                  names=['filename', 'scan', 'cluster'],\n",
    "                                  usecols=['scan', 'cluster'])\n",
    "                      .dropna(how='all'))\n",
    "    cluster_labels.set_index('scan', inplace=True)\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_mscluster(dir_name, ids=None):\n",
    "    clusters, cluster_i = [], -1\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith('.clust'):\n",
    "            with open(os.path.join(dir_name, filename)) as f_in:\n",
    "                for line in f_in:\n",
    "                    if line.startswith('mscluster'):\n",
    "                        cluster_i += 1\n",
    "                    elif not line.isspace():\n",
    "                        splits = line.split('\\t')\n",
    "                        spectrum_i = int(splits[2])\n",
    "                        clusters.append((spectrum_i, cluster_i))\n",
    "    cluster_labels = (pd.DataFrame(clusters, columns=['index', 'cluster'])\n",
    "                      .set_index('index').sort_index())\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "    \n",
    "    \n",
    "def get_clusters_mscrush(dir_name, ids=None):\n",
    "    cluster_labels = []\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith('.txt'):\n",
    "            clusters_file = pd.read_csv(os.path.join(dir_name, filename),\n",
    "                                        sep='\\t')\n",
    "            clusters_file['Titles'] = clusters_file['Titles'].str.split('|')\n",
    "            clusters_file = clusters_file.explode('Titles')\n",
    "            filenames_scans = clusters_file['Titles'].str.split(':')\n",
    "            clusters_file['filename'] = filenames_scans.str[0]\n",
    "            clusters_file['scan'] = filenames_scans.str[-1].astype(int)\n",
    "            clusters_file = clusters_file.rename(columns={'ID': 'cluster'})\n",
    "            clusters_file = clusters_file[['filename', 'scan', 'cluster']]\n",
    "            if len(clusters_file) > 0:\n",
    "                if len(cluster_labels) > 0:\n",
    "                    clusters_file['cluster'] += cluster_labels[-1].iat[-1, 2] + 1\n",
    "                cluster_labels.append(clusters_file)\n",
    "    cluster_labels = pd.concat(cluster_labels, ignore_index=True)\n",
    "    cluster_labels['filename'] += '.gz'\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels, ids,\n",
    "                                   'left', ['filename', 'scan'])\n",
    "                           .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = \\\n",
    "            cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_spectracluster(filename, ids=None):\n",
    "    identifiers, clusters, cluster_i = [], [], -1\n",
    "    with open(filename) as f_in:\n",
    "        for line in f_in:\n",
    "            if line.startswith('=Cluster='):\n",
    "                cluster_i += 1\n",
    "            elif line.startswith('SPEC'):\n",
    "                start_i = line.find('#id=index=') + len('#id=index=')\n",
    "                stop_i = line.find('#title', start_i)\n",
    "                spectrum_i = int(line[start_i:stop_i]) - 1\n",
    "                clusters.append((spectrum_i, cluster_i))\n",
    "    cluster_labels = (pd.DataFrame(clusters, columns=['index', 'cluster'])\n",
    "                      .set_index('index').sort_index())\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_gleams(filename_clusters, ids=None):\n",
    "    cluster_labels = pd.DataFrame({'cluster': np.load(filename_clusters)})\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### falcon\n",
    "\n",
    "falcon hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `eps`: maximum cosine distance between two spectra for them to be considered as neighbors of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_falcon = {0: 0.01, 1: 0.05, 2: 0.10, 3: 0.15, 4: 0.20, 5: 0.25, 6: 0.30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_falcon = os.path.join(cluster_dir, split_i, 'falcon')\n",
    "    if not os.path.exists(dir_falcon):\n",
    "        os.makedirs(dir_falcon)  \n",
    "    # falcon clustering.\n",
    "    for i, eps in hp_falcon.items():\n",
    "        logging.info('falcon run %d (eps=%.2f)', i + 1, eps)\n",
    "        filename = os.path.join(dir_falcon, f'clusters_{i}.csv.xz')\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"falcon \\\n",
    "            {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "            {os.path.join(cluster_dir, split_i, \"falcon\")}/clusters_{i} \\\n",
    "            --precursor_tol 10 ppm \\\n",
    "            --fragment_tol 0.05 \\\n",
    "            --eps {eps} \\\n",
    "            --min_intensity 0.1 \\\n",
    "            --scaling root\"\"\"\n",
    "        if not os.path.isfile(filename):\n",
    "            ! eval {cmd}\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_falcon(filename, metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'falcon', (eps,),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaRaCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaRaCluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `--pvalThreshold <X>` / `--clusterThresholds <X>`: p-value threshold to merge spectra into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_maracluster = {0: -3.0, 1: -5.0, 2: -10.0, 3: -15.0, 4: -20.0, 5: -25.0,\n",
    "                  6: -30.0, 7: -50.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_maracluster = os.path.join(cluster_dir, split_i, 'maracluster')\n",
    "    if not os.path.exists(dir_maracluster):\n",
    "        os.makedirs(dir_maracluster)    \n",
    "    # MaRaCluster preprocessing.\n",
    "    cmd = f\"\"\"realpath {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "        > {dir_maracluster}/files.txt\"\"\"\n",
    "    ! eval {cmd}\n",
    "    # MaRaCluster clustering.\n",
    "    for i, pval_threshold in hp_maracluster.items():\n",
    "        logging.info('MaRaCluster run %d (p-value threshold=%.1f)',\n",
    "                     i + 1, pval_threshold)\n",
    "        filename_orig = os.path.join(\n",
    "            dir_maracluster,\n",
    "            f'clusters_{i}.clusters_p{abs(int(pval_threshold))}.tsv')\n",
    "        filename = os.path.join(dir_maracluster, f'clusters_{i}.tsv')\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"$GLEAMS_HOME/bin/maracluster-v1-01-linux-amd64/bin/maracluster batch \\\n",
    "            --batch {dir_maracluster}/files.txt \\\n",
    "            --output-folder {dir_maracluster} \\\n",
    "            --precursorTolerance 10ppm \\\n",
    "            --pvalThreshold {pval_threshold} \\\n",
    "            --clusterThresholds {pval_threshold} \\\n",
    "            --prefix clusters_{i}\"\"\"\n",
    "        if not os.path.isfile(filename):\n",
    "            ! eval {cmd} && \\\n",
    "                mv {filename_orig} {filename} && \\\n",
    "                rm {dir_maracluster}/*.dat && \\\n",
    "                rm {dir_maracluster}/*.dat.pvalue_tree.tsv && \\\n",
    "                rm {dir_maracluster}/*.dat_file_list.txt && \\\n",
    "                rm {dir_maracluster}/overlap.pvalue_tree.tsv\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_maracluster(filename, metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'MaRaCluster', (pval_threshold,),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS-Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS-Cluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `--mixture-prob <X>`: the probability wrongfully adding a spectrum to a cluster (default X=0.05)\n",
    "- `--num-rounds <X>`: determines how many rounds are used for the hierarchical clustering (default X=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mscluster = {0: 0.00001, 1: 0.0001, 2: 0.001, 3: 0.005, 4: 0.01, 5: 0.05,\n",
    "                6: 0.1}\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_mscluster = os.path.join(cluster_dir, split_i, 'mscluster')\n",
    "    if not os.path.exists(dir_mscluster):\n",
    "        os.makedirs(dir_mscluster)    \n",
    "    # MS-Cluster preprocessing.\n",
    "    cmd = f\"\"\"realpath {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "        > {dir_mscluster}/mscluster_spec_list.txt\"\"\"\n",
    "    ! eval {cmd}\n",
    "    # MS-Cluster clustering.\n",
    "    for i, mixture_prob in hp_mscluster.items():\n",
    "        logger.info('MS-Cluster run %d (mixture-prob=%.5f ; num-rounds=%d)',\n",
    "                    i + 1, mixture_prob, rounds)\n",
    "        dir_mscluster_i = os.path.join(dir_mscluster, f'clusters_{i}')\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"$GLEAMS_HOME/bin/MsCluster/MsCluster \\\n",
    "            --model LTQ_TRYP \\\n",
    "            --list {dir_mscluster}/mscluster_spec_list.txt \\\n",
    "            --output-name mscluster \\\n",
    "            --output-file-size 100000000 \\\n",
    "            --tmp-dir {os.path.join(dir_mscluster, \"tmp\")} \\\n",
    "            --out-dir {dir_mscluster_i} \\\n",
    "            --model-dir $GLEAMS_HOME/bin/MsCluster/Models \\\n",
    "            --memory-gb 300 \\\n",
    "            --fragment-tolerance 0.05 \\\n",
    "            --precursor-ppm 10 \\\n",
    "            --assign-charges \\\n",
    "            --mixture-prob {mixture_prob} \\\n",
    "            --num-rounds {rounds} \\\n",
    "            --keep-dataset-idx\"\"\"\n",
    "        if not os.path.exists(dir_mscluster_i):\n",
    "            ! eval {cmd}\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_mscluster(\n",
    "            os.path.join(dir_mscluster_i, 'clust'), metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'MS-Cluster', (mixture_prob, rounds),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectra-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectra-cluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `-rounds <arg>`: number of clustering rounds to use.\n",
    "- `-threshold_end <arg>`: (lowest) final clustering threshold\n",
    "- `-threshold_start <arg>`: (highest) starting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_spectracluster = {0: 0.99999, 1: 0.9999, 2: 0.999, 3: 0.99, 4: 0.95,\n",
    "                     5: 0.9, 6: 0.8}\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_spectracluster = os.path.join(cluster_dir, split_i, 'spectra-cluster')\n",
    "    if not os.path.exists(os.path.join(dir_spectracluster, 'tmp')):\n",
    "        os.makedirs(os.path.join(dir_spectracluster, 'tmp'))\n",
    "    # spectra-cluster clustering.\n",
    "    for i, threshold_end in hp_spectracluster.items():\n",
    "        logger.info('spectra-cluster run %d (threshold_end=%.5f ; rounds=%d)',\n",
    "                    i + 1, threshold_end, rounds)\n",
    "        filename = os.path.join(dir_spectracluster, f'clusters_{i}.txt')\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"java -jar $GLEAMS_HOME/bin/spectra-cluster/spectra-cluster-cli-1.1.2.jar \\\n",
    "            {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "            -binary_directory {dir_spectracluster}/tmp \\\n",
    "            -fast_mode \\\n",
    "            -fragment_tolerance 0.05 \\\n",
    "            -keep_binary_files \\\n",
    "            -major_peak_jobs $(nproc --all) \\\n",
    "            -output_path {filename} \\\n",
    "            -precursor_tolerance 10 \\\n",
    "            -precursor_tolerance_unit ppm \\\n",
    "            -reuse_binary_files \\\n",
    "            -rounds {rounds} \\\n",
    "            -threshold_end {threshold_end} \\\n",
    "            -threshold_start 1.0 \\\n",
    "            -x_disable_mgf_comments\"\"\"\n",
    "        if not os.path.isfile(filename):\n",
    "            ! eval {cmd}\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_spectracluster(filename, metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'spectra-cluster',\n",
    "                                (threshold_end, rounds),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLEAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_gleams = {\n",
    "    0: ('complete', 0.1), 1: ('complete', 0.2), 2: ('complete', 0.3),\n",
    "    3: ('complete', 0.4), 4: ('complete', 0.5), 5: ('complete', 0.6),\n",
    "    6: ('complete', 0.7), 7: ('complete', 0.8),\n",
    "    8: ('single', 0.05), 9: ('single', 0.10), 10: ('single', 0.15),\n",
    "    11: ('single', 0.20), 12: ('single', 0.25),\n",
    "    13: ('average', 0.1), 14: ('average', 0.2), 15: ('average', 0.3),\n",
    "    16: ('average', 0.4), 17: ('average', 0.5), 18: ('average', 0.6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    dir_gleams = os.path.join(cluster_dir, split_i, 'gleams')\n",
    "    if not os.path.exists(dir_gleams):\n",
    "        os.makedirs(dir_gleams)\n",
    "    embed_filename = os.path.join(dir_gleams, 'embed_cluster_comparison.npy')\n",
    "    metadata_filename = os.path.join(cluster_dir, split_i,\n",
    "                                     'cluster_comparison.parquet')\n",
    "    metadata = pd.read_parquet(metadata_filename)\n",
    "    # Extract the relevant entries from all (previously computed) embeddings.\n",
    "    if not os.path.isfile(embed_filename):\n",
    "        embed_idx = pd.merge(\n",
    "            metadata, pd.read_parquet(\n",
    "                os.path.join(\n",
    "                    os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "                    f'embed_{config.massivekb_task_id}_{split}.parquet'),\n",
    "                columns=['dataset', 'filename', 'scan']).reset_index(),\n",
    "            'left', ['dataset', 'filename', 'scan'])['index'].values\n",
    "        embeddings = np.load(os.path.join(\n",
    "            os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "            f'embed_{config.massivekb_task_id}_{split}.npy'), mmap_mode='r')\n",
    "        np.save(embed_filename, embeddings[embed_idx])\n",
    "    # GLEAMS clustering.\n",
    "    for i, (linkage, distance_threshold) in hp_gleams.items():\n",
    "        logger.info('GLEAMS run %d (%s linkage, distance_threshold=%.2f)',\n",
    "                    i + 1, linkage, distance_threshold)\n",
    "        cluster_filename = os.path.join(\n",
    "            dir_gleams, f'clusters_cluster_comparison_{i}.npy')\n",
    "        # Execute clustering.\n",
    "        if not os.path.isfile(cluster_filename):\n",
    "            cluster.cluster(\n",
    "                embed_filename, metadata_filename, cluster_filename,\n",
    "                config.precursor_tol_mass, config.precursor_tol_mode,\n",
    "                linkage, distance_threshold, config.charges)\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_gleams(cluster_filename, metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, f'GLEAMS {linkage} linkage',\n",
    "                                (distance_threshold,),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(performance, columns=[\n",
    "    'split_i', 'tool', 'hyperparameters',\n",
    "    'min_cluster_size', 'max_cluster_size',\n",
    "    'num_clustered', 'num_noise',\n",
    "    'prop_clustered', 'prop_clustered_incorrect',\n",
    "    'homogeneity', 'completeness'])\n",
    "performance.to_csv('cluster_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.read_csv('cluster_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, axes = plt.subplots(1, 2, figsize=(width * 2, height))\n",
    "\n",
    "for tool in ('GLEAMS average linkage', 'falcon', 'MaRaCluster',\n",
    "             'MS-Cluster', 'spectra-cluster'):\n",
    "    tool_performance = (performance[performance['tool'] == tool]\n",
    "                        .sort_values('prop_clustered_incorrect'))\n",
    "    if tool.startswith('GLEAMS'):\n",
    "        tool = 'GLEAMS'\n",
    "    axes[0].plot(\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['prop_clustered_incorrect'].mean()),\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['prop_clustered'].mean()),\n",
    "        marker='o', label=tool)\n",
    "    axes[1].plot(\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['prop_clustered_incorrect'].mean()),\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['completeness'].mean()),\n",
    "        marker='o', label=tool)\n",
    "\n",
    "axes[0].set_xlim(0, 0.05)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[1].set_xlim(0, 0.05)\n",
    "axes[1].set_ylim(0.7, 1)\n",
    "\n",
    "axes[0].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[0].yaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[1].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "\n",
    "axes[0].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[0].set_ylabel('Clustered spectra')\n",
    "axes[1].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[1].set_ylabel('Completeness')\n",
    "\n",
    "axes[0].legend(loc='upper left', frameon=False)\n",
    "axes[1].legend(loc='upper left', frameon=False)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "for ax in axes:\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "plt.savefig('cluster_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, axes = plt.subplots(1, 2, figsize=(width * 2, height))\n",
    "\n",
    "for tool in ('GLEAMS complete linkage', 'GLEAMS single linkage',\n",
    "             'GLEAMS average linkage', 'GLEAMS DBSCAN'):\n",
    "    tool_performance = (performance[performance['tool'] == tool]\n",
    "                        .sort_values('prop_clustered_incorrect'))\n",
    "    tool = tool[7:]\n",
    "    axes[0].plot(\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['prop_clustered_incorrect'].mean()),\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['prop_clustered'].mean()),\n",
    "        marker='o', label=tool, zorder=0.9)\n",
    "    axes[1].plot(\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['prop_clustered_incorrect'].mean()),\n",
    "        (tool_performance.groupby('hyperparameters', sort=False)\n",
    "         ['completeness'].mean()),\n",
    "        marker='o', label=tool, zorder=0.9)\n",
    "\n",
    "axes[0].set_xlim(0, 0.05)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[1].set_xlim(0, 0.05)\n",
    "axes[1].set_ylim(0.7, 1)\n",
    "\n",
    "axes[0].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[0].yaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[1].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "\n",
    "axes[0].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[0].set_ylabel('Clustered spectra')\n",
    "axes[1].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[1].set_ylabel('Completeness')\n",
    "\n",
    "axes[0].legend(loc='upper left', frameon=False)\n",
    "axes[1].legend(loc='upper left', frameon=False)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "for ax in axes:\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "plt.savefig('cluster_comparison_gleams.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster size and number of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use the first split for simplicity.\n",
    "# The clustering results are very similar across the splits.\n",
    "split_i = 0\n",
    "performance_split = (performance[performance['split_i'] == split_i]\n",
    "                     .reset_index().copy())\n",
    "performance_split.loc[\n",
    "    (performance_split['tool'].str.startswith('GLEAMS')) &\n",
    "    (performance_split['tool'] != 'GLEAMS average linkage'),\n",
    "    'prop_clustered_incorrect'] = np.inf\n",
    "performance_split.loc[performance_split['tool'].str.startswith('GLEAMS'),\n",
    "                      'tool'] = 'GLEAMS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"best\" performing runs (low number of incorrectly clustered spectra).\n",
    "target_incorrect, tool_index = 0.01, {}\n",
    "for tool, tool_performance in \\\n",
    "        ((performance_split.set_index('tool')['prop_clustered_incorrect']\n",
    "          - target_incorrect).abs().reset_index().groupby('tool')):\n",
    "    if tool in ('GLEAMS complete linkage', 'GLEAMS single linkage',\n",
    "                'GLEAMS DBSCAN', 'msCRUSH'):\n",
    "        continue\n",
    "    idxmin = (tool_performance['prop_clustered_incorrect'] ==\n",
    "              tool_performance['prop_clustered_incorrect'].min())\n",
    "    tool_index[tool] = (np.where(idxmin)[0][0],\n",
    "                        tool_performance[idxmin].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_split.loc[[idx[1] for idx in tool_index.values()]].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clustering results from the different tools.\n",
    "metadata = pd.read_parquet(os.path.join(\n",
    "    cluster_dir, str(split_i), 'cluster_comparison.parquet'))\n",
    "\n",
    "tool_clusters = {\n",
    "    'GLEAMS': get_clusters_gleams(\n",
    "        os.path.join(cluster_dir, str(split_i), 'gleams',\n",
    "                     f'clusters_cluster_comparison_'\n",
    "                     f'{tool_index[\"GLEAMS\"][0]}.npy'),\n",
    "        metadata),\n",
    "    'falcon': get_clusters_falcon(\n",
    "        os.path.join(cluster_dir, str(split_i), 'falcon',\n",
    "                     f'clusters_{tool_index[\"falcon\"][0]}.csv.xz'),\n",
    "        metadata),\n",
    "    'MaRaCluster': get_clusters_maracluster(\n",
    "        os.path.join(cluster_dir, str(split_i), 'maracluster',\n",
    "                     f'clusters_{tool_index[\"MaRaCluster\"][0]}.tsv.xz'),\n",
    "        metadata),\n",
    "    'MS-Cluster': get_clusters_mscluster(\n",
    "        os.path.join(cluster_dir, str(split_i), 'mscluster',\n",
    "                     f'clusters_{tool_index[\"MS-Cluster\"][0]}', 'clust'),\n",
    "        metadata),\n",
    "    'spectra-cluster': get_clusters_spectracluster(\n",
    "        os.path.join(cluster_dir, str(split_i), 'spectra-cluster',\n",
    "                     f'clusters_{tool_index[\"spectra-cluster\"][0]}.txt'),\n",
    "        metadata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove singleton and noise clusters.\n",
    "min_cluster_size, max_cluster_size = min_cluster_sizes[0]\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = clusters['cluster'].value_counts(dropna=False)\n",
    "    if -1 in cluster_map.index:\n",
    "        cluster_map = cluster_map.drop(index=-1)\n",
    "    cluster_map = (cluster_map.to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "    \n",
    "    # Only consider clusters with specific minimum (inclusive) and/or\n",
    "    # maximum (exclusive) size.\n",
    "    cluster_counts = clusters['cluster'].value_counts(dropna=False)\n",
    "    if min_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts < min_cluster_size].index), 'cluster'] = -1\n",
    "    if max_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts >= max_cluster_size].index), 'cluster'] = -1\n",
    "    \n",
    "    tool_clusters[tool] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster sizes.\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    cluster_counts = (clusters['cluster']\n",
    "                      .value_counts(dropna=False)\n",
    "                      .to_frame()\n",
    "                      .reset_index()\n",
    "                      .rename(columns={'index': 'cluster', 'cluster': 'size'}))\n",
    "    tool_clusters[tool] = pd.merge(clusters, cluster_counts, on='cluster')\n",
    "cluster_sizes = {tool: clusters[clusters['cluster'] != -1]['size'].values\n",
    "                 for tool, clusters in tool_clusters.items()}\n",
    "_ = joblib.dump(cluster_sizes, 'cluster_comparison_size.joblib')\n",
    "# Count number of datasets per cluster.\n",
    "n_datasets = {tool: (clusters[clusters['cluster'] != -1].groupby('cluster')\n",
    "                     ['dataset'].nunique().values)\n",
    "              for tool, clusters in tool_clusters.items()}\n",
    "_ = joblib.dump(n_datasets, 'cluster_comparison_n_datasets.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of clusters / clustered spectra / incorrectly clustered spectra:')\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    print(f'- {tool}: {clusters.loc[clusters[\"cluster\"] != -1, \"cluster\"].nunique():,d} / '\n",
    "          f'{performance_split.loc[tool_index[tool][1], \"num_clustered\"]:,} / '\n",
    "          f'{performance_split.loc[tool_index[tool][1], \"prop_clustered_incorrect\"]:.2%}')\n",
    "print('Cluster sizes (median ± IQR):')\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    cluster_size_non_noise = clusters[clusters[\"cluster\"] != -1][\"size\"]\n",
    "    print(f'- {tool}: {np.median(cluster_size_non_noise):.0f} ± '\n",
    "          f'{scipy.stats.iqr(cluster_size_non_noise):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "for tool, cluster_size in cluster_sizes.items():\n",
    "    sns.ecdfplot(cluster_size,\n",
    "                 stat='proportion', complementary=True, ax=ax,\n",
    "                 label=tool)\n",
    "    \n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(0., 1.01)\n",
    "\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1))\n",
    "\n",
    "ax.set_xlabel('Minimum cluster size')\n",
    "ax.set_ylabel('Proportion of clustered spectra')\n",
    "\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('cluster_comparison_size.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "bins = np.arange(1, max([n.max() for n in n_datasets.values()]) + 2, 1)\n",
    "for tool, tool_n_datasets in n_datasets.items():\n",
    "    hist = np.histogram(tool_n_datasets, bins)\n",
    "    mask = hist[0] > 0\n",
    "    ax.plot(bins[:-1][mask], hist[0][mask], marker='o', label=tool)\n",
    "\n",
    "ax.xaxis.set_major_locator(mticker.FixedLocator([4, 8, 12, 16, 20]))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel('Number of different datasets')\n",
    "ax.set_ylabel('Number of clusters')\n",
    "\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('cluster_comparison_n_datasets.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
