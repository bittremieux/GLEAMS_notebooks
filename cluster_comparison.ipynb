{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['GLEAMS_HOME'] = os.path.join(os.environ['HOME'],\n",
    "                                         'Projects/gleams')\n",
    "# Make sure all code is in the PATH.\n",
    "sys.path.append(\n",
    "    os.path.normpath(os.path.join(os.environ['GLEAMS_HOME'], 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import EfficiencyWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=EfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import itertools\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyteomics\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.metrics import homogeneity_score, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "from gleams import logger as glogger\n",
    "glogger.init()\n",
    "# Initialize all random seeds before importing any packages.\n",
    "from gleams import rndm\n",
    "rndm.set_seeds()\n",
    "\n",
    "from gleams import config\n",
    "from gleams.cluster import cluster\n",
    "from gleams.feature import feature, spectrum\n",
    "from gleams.ms_io import ms_io\n",
    "from gleams.nn import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('gleams')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)    # Single-column figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $GLEAMS_HOME/notebooks/cluster_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'notebooks', 'cluster_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectra_from_file(dataset, filename, scans):\n",
    "    logger.debug('Process file %s/%s', dataset, filename)\n",
    "    peak_filename = os.path.join(os.environ['GLEAMS_HOME'], 'data', 'peak',\n",
    "                                 dataset, filename)\n",
    "    if os.path.isfile(peak_filename):\n",
    "        return [spec for spec in ms_io.get_spectra(peak_filename, scans)\n",
    "                if spectrum.preprocess(copy.deepcopy(spec),\n",
    "                                       config.fragment_mz_min,\n",
    "                                       config.fragment_mz_max).is_valid]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "max_spectra_per_split = 20_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_parquet(\n",
    "    os.path.join(os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "                 f'embed_{config.massivekb_task_id}_{split}.parquet'))\n",
    "datasets_splits = np.array_split(\n",
    "    datasets.sample(frac=1),\n",
    "    max(3, len(datasets) // max_spectra_per_split + 1))\n",
    "\n",
    "psms = pd.read_parquet(\n",
    "    os.path.join(os.environ['GLEAMS_HOME'], 'data', 'metadata',\n",
    "                 f'massivekb_ids_{config.massivekb_task_id}.parquet'),\n",
    "    columns=['dataset', 'filename', 'scan', 'sequence'])\n",
    "    \n",
    "for split_i, datasets_split in enumerate(datasets_splits):\n",
    "    cluster_dir_split = os.path.join(cluster_dir, str(split_i))\n",
    "    if not os.path.exists(cluster_dir_split):\n",
    "        os.makedirs(cluster_dir_split)\n",
    "    filename_mgf = os.path.join(cluster_dir_split, 'cluster_comparison.mgf')\n",
    "    filename_metadata = os.path.join(cluster_dir_split,\n",
    "                                     'cluster_comparison.parquet')\n",
    "    if not os.path.isfile(filename_metadata):\n",
    "        logger.info('Partition %d/%d: Export spectra to be clustered to MGF '\n",
    "                    'file', split_i + 1, len(datasets_splits))\n",
    "        if os.path.isfile(filename_mgf):\n",
    "            os.remove(filename_mgf)\n",
    "        spectrum_idx = []\n",
    "        dataset_filename_scans = (datasets_split\n",
    "                                  .groupby(['dataset', 'filename'])['scan']\n",
    "                                  .apply(list).reset_index())\n",
    "        for dataset, filename, spectra in tqdm.tqdm(\n",
    "                zip(dataset_filename_scans['dataset'],\n",
    "                    dataset_filename_scans['filename'],\n",
    "                    joblib.Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "                        joblib.delayed(get_spectra_from_file)\n",
    "                        (dataset, filename, scans)\n",
    "                        for dataset, filename, scans in zip(\n",
    "                            dataset_filename_scans['dataset'],\n",
    "                            dataset_filename_scans['filename'],\n",
    "                            dataset_filename_scans['scan']))),\n",
    "                desc='Files processed', total=len(dataset_filename_scans)):\n",
    "            if spectra is not None:\n",
    "                spectra_dicts = []\n",
    "                for spec in spectra:\n",
    "                    if (config.charges[0] <= spec.precursor_charge\n",
    "                            <= config.charges[1]):\n",
    "                        spectra_dicts.append(\n",
    "                            {'m/z array': spec.mz,\n",
    "                            'intensity array': spec.intensity,\n",
    "                            'params': {\n",
    "                                'TITLE': len(spectrum_idx),\n",
    "                                'RTINSECONDS': spec.retention_time,\n",
    "                                'PEPMASS': spec.precursor_mz,\n",
    "                                'CHARGE': f'{spec.precursor_charge}+'}})\n",
    "                        spectrum_idx.append(\n",
    "                            (dataset, filename, int(spec.identifier),\n",
    "                             spec.precursor_charge, spec.precursor_mz))\n",
    "                with open(filename_mgf, 'a') as f:\n",
    "                    pyteomics.mgf.write(spectra_dicts, f)\n",
    "        metadata = pd.merge(\n",
    "            pd.DataFrame(spectrum_idx, columns=['dataset', 'filename', 'scan',\n",
    "                                                'charge', 'mz']),\n",
    "            psms, 'left', ['dataset', 'filename', 'scan'])\n",
    "        metadata['sequence'] = (metadata['sequence'].str.replace('I', 'L')\n",
    "                                + '/' + metadata['charge'].astype(str))\n",
    "        metadata.to_parquet(filename_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_sizes = [(2, None), (5, None), (10, None), (50, None)]\n",
    "min_peptide_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_majority_label_mismatch(labels):\n",
    "    labels_assigned = labels.dropna()\n",
    "    if len(labels_assigned) <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(labels_assigned) - labels_assigned.value_counts().iat[0]\n",
    "\n",
    "\n",
    "def evaluate_clusters(clusters, min_cluster_size=None, max_cluster_size=None):\n",
    "    clusters = clusters.copy()\n",
    "    # Only consider clusters with specific minimum (inclusive) and/or\n",
    "    # maximum (exclusive) size.\n",
    "    cluster_counts = clusters['cluster'].value_counts(dropna=False)\n",
    "    if min_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts < min_cluster_size].index), 'cluster'] = -1\n",
    "    if max_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts >= max_cluster_size].index), 'cluster'] = -1\n",
    "\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = (clusters['cluster'].value_counts(dropna=False)\n",
    "                   .drop(index=-1).to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "    num_clusters = clusters['cluster'].max() + 1\n",
    "\n",
    "    # Reassign noise points to singleton clusters.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_noise = noise_mask.sum()\n",
    "    clusters.loc[noise_mask, 'cluster'] = np.arange(\n",
    "        num_clusters, num_clusters + num_noise)\n",
    "\n",
    "    # Compute cluster evaluation measures.\n",
    "    prop_clustered = (len(clusters) - num_noise) / len(clusters)\n",
    "\n",
    "    clusters_ident = clusters.dropna(subset=['sequence'])\n",
    "    clusters_ident_non_noise = (clusters[~noise_mask]\n",
    "                                .dropna(subset=['sequence']))\n",
    "\n",
    "    # The number of incorrectly clustered spectra is the number of PSMs that\n",
    "    # differ from the majority PSM. Unidentified spectra are not considered.\n",
    "    prop_clustered_incorrect = sum(joblib.Parallel(n_jobs=-1)(\n",
    "        joblib.delayed(_count_majority_label_mismatch)(clust['sequence'])\n",
    "        for _, clust in clusters[~noise_mask].groupby('cluster')))\n",
    "    prop_clustered_incorrect /= len(clusters_ident_non_noise)\n",
    "\n",
    "    # Homogeneity measures whether clusters contain only identical PSMs.\n",
    "    # This is only evaluated on non-noise points, because the noise cluster\n",
    "    # is highly non-homogeneous by definition.\n",
    "    homogeneity = homogeneity_score(clusters_ident_non_noise['sequence'],\n",
    "                                    clusters_ident_non_noise['cluster'])\n",
    "    # Completeness measures whether identical PSMs are assigned to the same\n",
    "    # cluster.\n",
    "    # This is evaluated on all PSMs, including those clustered as noise.\n",
    "    completeness = completeness_score(clusters_ident['sequence'],\n",
    "                                      clusters_ident['cluster'])\n",
    "\n",
    "    return (len(clusters) - num_noise, num_noise,\n",
    "            prop_clustered, prop_clustered_incorrect,\n",
    "            homogeneity, completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clusters_mscluster(dir_clusters, sequences):\n",
    "    cluster_labels, cluster_i = np.full(len(sequences), -1), -1\n",
    "    for filename in os.listdir(dir_clusters):\n",
    "        if filename.endswith('.clust'):\n",
    "            with open(os.path.join(dir_clusters, filename)) as f_in:\n",
    "                for line in f_in:\n",
    "                    if line.startswith('mscluster'):\n",
    "                        cluster_i += 1\n",
    "                    elif not line.isspace():\n",
    "                        cluster_labels[int(line.split('\\t')[2])] = cluster_i\n",
    "    return pd.DataFrame({'sequence': sequences, 'cluster': cluster_labels})\n",
    "\n",
    "\n",
    "def _get_clusters_spectracluster(filename_clusters, sequences):\n",
    "    cluster_labels, cluster_i = np.full(len(sequences), -1), -1\n",
    "    with open(filename_clusters) as f_in:\n",
    "        for line in f_in:\n",
    "            if line.startswith('=Cluster='):\n",
    "                cluster_i += 1\n",
    "            elif line.startswith('SPEC'):\n",
    "                cluster_labels[\n",
    "                    int(line[line.find('#id=index=') + len('#id=index='):\n",
    "                             line.find('#title')]) - 1] = cluster_i\n",
    "    return pd.DataFrame({'sequence': sequences, 'cluster': cluster_labels})\n",
    "\n",
    "\n",
    "def _get_clusters_gleams(filename_clusters, sequences):\n",
    "    return pd.DataFrame({'sequence': sequences,\n",
    "                         'cluster': np.load(filename_clusters)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param = {\n",
    "    'MS-Cluster': list(itertools.product(\n",
    "        [0.0001, 0.001, 0.01, 0.1], [3, 10])),\n",
    "    'spectra-cluster': list(itertools.product(\n",
    "        [0.9999, 0.999, 0.99, 0.95, 0.9], [3, 10])),\n",
    "    'GLEAMS': list(itertools.product(np.arange(0.02, 0.08, 0.005), [2]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS-Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS-Cluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `--mixture-prob <X>`: the probability wrongfully adding a spectrum to a cluster (default X=0.05)\n",
    "- `--num-rounds <X>`: determines how many rounds are used for the hierarchical clustering (default X=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_mscluster = os.path.join(cluster_dir, split_i, 'mscluster')\n",
    "    # MS-Cluster preprocessing.\n",
    "    if not os.path.exists(dir_mscluster):\n",
    "        os.makedirs(dir_mscluster)\n",
    "    cmd = f\"\"\"ls {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "        > {dir_mscluster}/mscluster_spec_list.txt\"\"\"\n",
    "    ! eval {cmd}\n",
    "    cmd = f\"\"\"$GLEAMS_HOME/bin/MsCluster/MsCluster \\\n",
    "        --model LTQ_TRYP \\\n",
    "        --list {dir_mscluster}/mscluster_spec_list.txt \\\n",
    "        --output-name mscluster \\\n",
    "        --tmp-dir {dir_mscluster}/dat \\\n",
    "        --out-dir {dir_mscluster} \\\n",
    "        --dat-only \\\n",
    "        --model-dir $GLEAMS_HOME/bin/MsCluster/Models \\\n",
    "        --keep-dat \\\n",
    "        --assign-charges\"\"\"\n",
    "    ! eval {cmd}\n",
    "    # MS-Cluster clustering.\n",
    "    for i, (mixture_prob, num_rounds) in enumerate(hyper_param['MS-Cluster']):\n",
    "        logger.info('MS-Cluster run %d (mixture-prob=%.4f ; num-rounds=%d)',\n",
    "                    i + 1, mixture_prob, num_rounds)\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"$GLEAMS_HOME/bin/MsCluster/MsCluster \\\n",
    "            --model LTQ_TRYP \\\n",
    "            --dat-list {dir_mscluster}/dat/mscluster_dat_list.txt \\\n",
    "            --output-name mscluster \\\n",
    "            --output-file-size {max_spectra_per_split} \\\n",
    "            --out-dir {dir_mscluster}/cluster_{i} \\\n",
    "            --model-dir $GLEAMS_HOME/bin/MsCluster/Models \\\n",
    "            --memory-gb {int(psutil.virtual_memory().available / (1024 ** 3))} \\\n",
    "            --fragment-tolerance 0.05 \\\n",
    "            --precursor-ppm 10 \\\n",
    "            --assign-charges \\\n",
    "            --mixture-prob {mixture_prob} \\\n",
    "            --num-rounds {num_rounds} \\\n",
    "            --keep-dataset-idx\"\"\"\n",
    "        if not os.path.isfile(os.path.join(dir_mscluster, f'cluster_{i}',\n",
    "                                           'mscluster_0_0_mgf_list.txt')):\n",
    "            ! eval {cmd}\n",
    "        # Account for failed MS-Cluster runs.\n",
    "        if not os.path.isfile(os.path.join(dir_mscluster, f'cluster_{i}',\n",
    "                                           'mscluster_0_0_mgf_list.txt')):\n",
    "            continue\n",
    "        # Evaluate clustering performance.\n",
    "        clusters = _get_clusters_mscluster(\n",
    "            os.path.join(dir_mscluster, f'cluster_{i}', 'clust'),\n",
    "            metadata['sequence'])\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(clusters, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append(('MS-Cluster', (mixture_prob, num_rounds),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectra-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectra-cluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `-rounds <arg>`: number of clustering rounds to use.\n",
    "- `-threshold_end <arg>`: (lowest) final clustering threshold\n",
    "- `-threshold_start <arg>`: (highest) starting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_spectracluster = os.path.join(cluster_dir, split_i, 'spectra-cluster')\n",
    "    if not os.path.exists(os.path.join(dir_spectracluster, 'tmp')):\n",
    "        os.makedirs(os.path.join(dir_spectracluster, 'tmp'))\n",
    "    # spectra-cluster clustering.\n",
    "    for i, (threshold_end, rounds) in enumerate(hyper_param['spectra-cluster']):\n",
    "        logger.info('spectra-cluster run %d (threshold_end=%.4f ; rounds=%d)',\n",
    "                    i + 1, threshold_end, rounds)\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"java -jar $GLEAMS_HOME/bin/spectra-cluster/spectra-cluster-cli-1.1.2.jar \\\n",
    "            {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "            -binary_directory {dir_spectracluster}/tmp \\\n",
    "            -fast_mode \\\n",
    "            -fragment_tolerance 0.05 \\\n",
    "            -keep_binary_files \\\n",
    "            -major_peak_jobs $(nproc --all) \\\n",
    "            -output_path {dir_spectracluster}/clusters_{i}.txt \\\n",
    "            -precursor_tolerance 10 \\\n",
    "            -precursor_tolerance_unit ppm \\\n",
    "            -reuse_binary_files \\\n",
    "            -rounds {rounds} \\\n",
    "            -threshold_end {threshold_end} \\\n",
    "            -threshold_start 1.0 \\\n",
    "            -x_disable_mgf_comments\"\"\"\n",
    "        if not os.path.isfile(os.path.join(dir_spectracluster,\n",
    "                                           f'clusters_{i}.txt')):\n",
    "            ! eval {cmd}\n",
    "        # Evaluate clustering performance.\n",
    "        clusters = _get_clusters_spectracluster(\n",
    "            os.path.join(dir_spectracluster, f'clusters_{i}.txt'),\n",
    "            metadata['sequence'])\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(clusters, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append(('spectra-cluster', (threshold_end, rounds),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLEAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLEAMS hyperparameters that influence the clustering quality are ([Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)):\n",
    "\n",
    "- `eps`: The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
    "- `min_samples`:     The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata_filename = os.path.join(cluster_dir, split_i,\n",
    "                                     'cluster_comparison.parquet')\n",
    "    metadata = pd.read_parquet(metadata_filename)\n",
    "    dir_gleams = os.path.join(cluster_dir, split_i, 'gleams')\n",
    "    embed_filename = os.path.join(dir_gleams, 'embed_cluster_comparison.npy')\n",
    "    dist_filename = os.path.join(dir_gleams, 'dist_cluster_comparison.npz')\n",
    "    clust_filename = os.path.join(dir_gleams, 'clusters_cluster_comparison.npy')\n",
    "    # GLEAMS preprocessing.\n",
    "    if not os.path.exists(dir_gleams):\n",
    "        os.makedirs(dir_gleams)\n",
    "    # Extract the relevant entries from all (previously computed) embeddings.\n",
    "    if not os.path.isfile(embed_filename):\n",
    "        embed_idx = pd.merge(\n",
    "            metadata, pd.read_parquet(\n",
    "                os.path.join(\n",
    "                    os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "                    f'embed_{config.massivekb_task_id}_{split}.parquet'),\n",
    "                columns=['dataset', 'filename', 'scan']).reset_index(),\n",
    "            'left', ['dataset', 'filename', 'scan'])['index'].values\n",
    "        embeddings = np.load(os.path.join(\n",
    "            os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "            f'embed_{config.massivekb_task_id}_{split}.npy'), mmap_mode='r')\n",
    "        np.save(embed_filename, embeddings[embed_idx])\n",
    "    # Compute pairwise distances.\n",
    "    if not os.path.isfile(dist_filename):\n",
    "        cluster.compute_pairwise_distances(\n",
    "            embed_filename, metadata_filename, config.charges)\n",
    "        os.rename(os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "                               'dist_cluster_comparison.npz'),\n",
    "                  dist_filename)\n",
    "        os.rename(os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "                               os.path.basename(metadata_filename)),\n",
    "                  metadata_filename)\n",
    "        os.rename(os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "                               os.path.basename(embed_filename)),\n",
    "                  embed_filename)\n",
    "        metadata = pd.read_parquet(metadata_filename)\n",
    "    # GLEAMS clustering.\n",
    "    for i, (eps, min_samples) in enumerate(hyper_param['GLEAMS']):\n",
    "        logger.info('GLEAMS run %d (eps=%.3f ; min_samples=%d)',\n",
    "                    i + 1, eps, min_samples)\n",
    "        if os.path.isfile(clust_filename):\n",
    "            os.remove(clust_filename)\n",
    "        config.eps, config.min_samples = eps, min_samples\n",
    "        cluster_filename_i = clust_filename.replace('.npy', f'_{i}.npy')\n",
    "        # Execute clustering.\n",
    "        if not os.path.isfile(cluster_filename_i):\n",
    "            cluster.cluster(dist_filename, metadata_filename)\n",
    "            # Rename file to retain clustering results.\n",
    "            os.rename(clust_filename, cluster_filename_i)\n",
    "        # Evaluate clustering performance.\n",
    "        clusters = _get_clusters_gleams(cluster_filename_i,\n",
    "                                        metadata['sequence'])\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(clusters, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append(('GLEAMS', (eps, min_samples),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))\n",
    "    # Clean up ANN indexes.\n",
    "    ann_dir = os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster', 'ann')\n",
    "    for filename in os.listdir(ann_dir):\n",
    "        if filename.startswith('ann_cluster_comparison_'):\n",
    "            os.remove(os.path.join(ann_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(performance, columns=[\n",
    "    'tool', 'hyperparameters',\n",
    "    'min_cluster_size', 'max_cluster_size',\n",
    "    'num_clustered', 'num_noise',\n",
    "    'prop_clustered', 'prop_clustered_incorrect',\n",
    "    'homogeneity', 'completeness'])\n",
    "performance.to_csv('cluster_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance = pd.read_csv('cluster_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pareto_frontier_idx(arr, threshold0=0, threshold1=0):\n",
    "    # Sort by the first column.\n",
    "    order = arr[:, 0].argsort()\n",
    "    arr_sorted = arr[order]\n",
    "    # Iteratively add points to the Pareto frontier.\n",
    "    pareto_idx = [0]\n",
    "    for i in range(1, arr_sorted.shape[0]):\n",
    "        if (arr_sorted[i, 0] > (arr_sorted[pareto_idx[-1], 0]\n",
    "                                + threshold0) and\n",
    "                arr_sorted[i, 1] > (arr_sorted[pareto_idx[-1], 1]\n",
    "                                    + threshold1)):\n",
    "            pareto_idx.append(i)\n",
    "    return order[pareto_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, axes = plt.subplots(1, 2, figsize=(width * 2, height))\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "# Number of clustered spectra and completeness.\n",
    "cluster_size = 2\n",
    "for tool in ('GLEAMS', 'MS-Cluster', 'spectra-cluster'):\n",
    "    tool_performance = performance[\n",
    "        (performance['tool'] == tool) &\n",
    "        (performance['min_cluster_size'] == cluster_size)]\n",
    "    axes[0].errorbar(\n",
    "        x=(tool_performance.groupby('hyperparameters')\n",
    "           ['prop_clustered_incorrect'].mean()),\n",
    "        y=(tool_performance.groupby('hyperparameters')\n",
    "           ['prop_clustered'].mean()),\n",
    "        xerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['prop_clustered_incorrect'].std()),\n",
    "        yerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['prop_clustered'].std()),\n",
    "        marker='o', label=tool)\n",
    "    axes[1].errorbar(\n",
    "        x=(tool_performance.groupby('hyperparameters')\n",
    "           ['prop_clustered_incorrect'].mean()),\n",
    "        y=(tool_performance.groupby('hyperparameters')\n",
    "           ['completeness'].mean()),\n",
    "        xerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['prop_clustered_incorrect'].std()),\n",
    "        yerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['completeness'].std()),\n",
    "        marker='o', label=tool)\n",
    "\n",
    "axes[0].set_xlim(0, 0.05)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[1].set_xlim(0, 0.05)\n",
    "axes[1].set_ylim(0.85, 1)\n",
    "\n",
    "axes[0].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[0].yaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[1].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "\n",
    "axes[0].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[0].set_ylabel('Clustered spectra')\n",
    "axes[1].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[1].set_ylabel('Completeness')\n",
    "\n",
    "axes[0].legend(loc='lower right', frameon=False)\n",
    "axes[1].legend(loc='lower right', frameon=False)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('cluster_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
