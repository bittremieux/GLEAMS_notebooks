{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['GLEAMS_HOME'] = os.path.join(os.environ['HOME'],\n",
    "                                         'Projects/gleams')\n",
    "# Make sure all code is in the PATH.\n",
    "sys.path.append(\n",
    "    os.path.normpath(os.path.join(os.environ['GLEAMS_HOME'], 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import EfficiencyWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=EfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyteomics\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "from gleams import logger as glogger\n",
    "glogger.init()\n",
    "# Initialize all random seeds before importing any packages.\n",
    "from gleams import rndm\n",
    "rndm.set_seeds()\n",
    "\n",
    "from gleams import config\n",
    "from gleams.cluster import cluster\n",
    "from gleams.feature import spectrum\n",
    "from gleams.ms_io import ms_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('gleams')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)    # Single-column figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $GLEAMS_HOME/notebooks/cluster_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'notebooks', 'cluster_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectra_from_file(dataset, filename, scans):\n",
    "    logger.debug('Process file %s/%s', dataset, filename)\n",
    "    peak_filename = os.path.join(os.environ['GLEAMS_HOME'], 'data', 'peak',\n",
    "                                 dataset, filename)\n",
    "    if os.path.isfile(peak_filename):\n",
    "        return [spec for spec in ms_io.get_spectra(peak_filename, scans)\n",
    "                if spectrum.preprocess(copy.deepcopy(spec),\n",
    "                                       config.fragment_mz_min,\n",
    "                                       config.fragment_mz_max).is_valid]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "max_spectra_per_split = 30_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_parquet(\n",
    "    os.path.join(os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "                 f'embed_{config.massivekb_task_id}_{split}.parquet'))\n",
    "datasets_splits = np.array_split(\n",
    "    datasets.sample(frac=1),\n",
    "    max(3, math.ceil(len(datasets) / max_spectra_per_split)))\n",
    "\n",
    "psms = pd.read_parquet(\n",
    "    os.path.join(os.environ['GLEAMS_HOME'], 'data', 'metadata',\n",
    "                 f'massivekb_ids_{config.massivekb_task_id}.parquet'),\n",
    "    columns=['dataset', 'filename', 'scan', 'sequence'])\n",
    "    \n",
    "for split_i, datasets_split in enumerate(datasets_splits):\n",
    "    cluster_dir_split = os.path.join(cluster_dir, str(split_i))\n",
    "    if not os.path.exists(cluster_dir_split):\n",
    "        os.makedirs(cluster_dir_split)\n",
    "    filename_mgf = os.path.join(cluster_dir_split, 'cluster_comparison.mgf')\n",
    "    filename_md = os.path.join(cluster_dir_split, 'cluster_comparison.parquet')\n",
    "    if not os.path.isfile(filename_md):\n",
    "        logger.info('Partition %d/%d: Export spectra to be clustered to MGF '\n",
    "                    'file', split_i + 1, len(datasets_splits))\n",
    "        if os.path.isfile(filename_mgf):\n",
    "            os.remove(filename_mgf)\n",
    "        spectrum_idx = []\n",
    "        dataset_filename_scans = (datasets_split\n",
    "                                  .groupby(['dataset', 'filename'])['scan']\n",
    "                                  .apply(list).reset_index())\n",
    "        for dataset, filename, spectra in tqdm.tqdm(\n",
    "                zip(dataset_filename_scans['dataset'],\n",
    "                    dataset_filename_scans['filename'],\n",
    "                    joblib.Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "                        joblib.delayed(get_spectra_from_file)\n",
    "                        (dataset, filename, scans)\n",
    "                        for dataset, filename, scans in zip(\n",
    "                            dataset_filename_scans['dataset'],\n",
    "                            dataset_filename_scans['filename'],\n",
    "                            dataset_filename_scans['scan']))),\n",
    "                desc='Files processed', total=len(dataset_filename_scans)):\n",
    "            if spectra is not None:\n",
    "                spectra_dicts = []\n",
    "                for spec in spectra:\n",
    "                    if (config.charges[0] <= spec.precursor_charge\n",
    "                            <= config.charges[1]):\n",
    "                        spectra_dicts.append(\n",
    "                            {'m/z array': spec.mz,\n",
    "                            'intensity array': spec.intensity,\n",
    "                            'params': {\n",
    "                                'TITLE': len(spectrum_idx),\n",
    "                                'RTINSECONDS': spec.retention_time,\n",
    "                                'PEPMASS': spec.precursor_mz,\n",
    "                                'CHARGE': f'{spec.precursor_charge}+'}})\n",
    "                        spectrum_idx.append(\n",
    "                            (dataset, filename, int(spec.identifier),\n",
    "                             spec.precursor_charge, spec.precursor_mz))\n",
    "                with open(filename_mgf, 'a') as f:\n",
    "                    pyteomics.mgf.write(spectra_dicts, f)\n",
    "        metadata = pd.merge(\n",
    "            pd.DataFrame(spectrum_idx, columns=['dataset', 'filename', 'scan',\n",
    "                                                'charge', 'mz']),\n",
    "            psms, 'left', ['dataset', 'filename', 'scan'])\n",
    "        metadata['sequence'] = metadata['sequence'].str.replace('I', 'L')\n",
    "        metadata.to_parquet(filename_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_sizes = [(2, None)] #, (5, None), (10, None), (50, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_majority_label_mismatch(labels):\n",
    "    labels_assigned = labels.dropna()\n",
    "    if len(labels_assigned) <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(labels_assigned) - labels_assigned.value_counts().iat[0]\n",
    "\n",
    "\n",
    "def evaluate_clusters(clusters, min_cluster_size=None, max_cluster_size=None,\n",
    "                      charges=None):\n",
    "    clusters = clusters.copy()\n",
    "    if charges is not None:\n",
    "        clusters = clusters[clusters['precursor_charge'].isin(charges)]\n",
    "\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = clusters['cluster'].value_counts(dropna=False)\n",
    "    if -1 in cluster_map.index:\n",
    "        cluster_map = cluster_map.drop(index=-1)\n",
    "    cluster_map = (cluster_map.to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "\n",
    "    # Reassign noise points to singleton clusters.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_clusters = clusters['cluster'].max() + 1\n",
    "    clusters.loc[noise_mask, 'cluster'] = np.arange(\n",
    "        num_clusters, num_clusters + noise_mask.sum())\n",
    "    \n",
    "    # Only consider clusters with specific minimum (inclusive) and/or\n",
    "    # maximum (exclusive) size.\n",
    "    cluster_counts = clusters['cluster'].value_counts(dropna=False)\n",
    "    if min_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts < min_cluster_size].index), 'cluster'] = -1\n",
    "    if max_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts >= max_cluster_size].index), 'cluster'] = -1\n",
    "\n",
    "    # Compute cluster evaluation measures.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_noise = noise_mask.sum()\n",
    "    num_clustered = len(clusters) - num_noise\n",
    "    prop_clustered = (len(clusters) - num_noise) / len(clusters)\n",
    "\n",
    "    clusters_ident = clusters.dropna(subset=['sequence'])\n",
    "    clusters_ident_non_noise = (clusters[~noise_mask]\n",
    "                                .dropna(subset=['sequence']))\n",
    "\n",
    "    # The number of incorrectly clustered spectra is the number of PSMs that\n",
    "    # differ from the majority PSM. Unidentified spectra are not considered.\n",
    "    prop_clustered_incorrect = sum(joblib.Parallel(n_jobs=-1)(\n",
    "        joblib.delayed(_count_majority_label_mismatch)(clust['sequence'])\n",
    "        for _, clust in clusters[~noise_mask].groupby('cluster')))\n",
    "    prop_clustered_incorrect /= len(clusters_ident_non_noise)\n",
    "\n",
    "    # Homogeneity measures whether clusters contain only identical PSMs.\n",
    "    # This is only evaluated on non-noise points, because the noise cluster\n",
    "    # is highly non-homogeneous by definition.\n",
    "    homogeneity = metrics.homogeneity_score(\n",
    "        clusters_ident_non_noise['sequence'],\n",
    "        clusters_ident_non_noise['cluster'])\n",
    "    \n",
    "    # Completeness measures whether identical PSMs are assigned to the same\n",
    "    # cluster.\n",
    "    # This is evaluated on all PSMs, including those clustered as noise.\n",
    "    completeness = metrics.completeness_score(\n",
    "        clusters_ident['sequence'], clusters_ident['cluster'])\n",
    "\n",
    "    return (num_clustered, num_noise,\n",
    "            prop_clustered, prop_clustered_incorrect,\n",
    "            homogeneity, completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_mscluster(dir_name, ids=None):\n",
    "    clusters, cluster_i = [], -1\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith('.clust'):\n",
    "            with open(os.path.join(dir_name, filename)) as f_in:\n",
    "                for line in f_in:\n",
    "                    if line.startswith('mscluster'):\n",
    "                        cluster_i += 1\n",
    "                    elif not line.isspace():\n",
    "                        splits = line.split('\\t')\n",
    "                        spectrum_i = int(splits[2])\n",
    "                        clusters.append((spectrum_i, cluster_i))\n",
    "    cluster_labels = (pd.DataFrame(clusters, columns=['index', 'cluster'])\n",
    "                      .set_index('index').sort_index())\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_spectracluster(filename, ids=None):\n",
    "    identifiers, clusters, cluster_i = [], [], -1\n",
    "    with open(filename) as f_in:\n",
    "        for line in f_in:\n",
    "            if line.startswith('=Cluster='):\n",
    "                cluster_i += 1\n",
    "            elif line.startswith('SPEC'):\n",
    "                start_i = line.find('#id=index=') + len('#id=index=')\n",
    "                stop_i = line.find('#title', start_i)\n",
    "                spectrum_i = int(line[start_i:stop_i]) - 1\n",
    "                clusters.append((spectrum_i, cluster_i))\n",
    "    cluster_labels = (pd.DataFrame(clusters, columns=['index', 'cluster'])\n",
    "                      .set_index('index').sort_index())\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_gleams(filename_clusters, ids=None):\n",
    "    cluster_labels = pd.DataFrame({'cluster': np.load(filename_clusters)})\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(ids, cluster_labels, 'right',\n",
    "                                   left_index=True, right_index=True)\n",
    "                          .dropna(subset=['charge']))\n",
    "        cluster_labels['charge'] = cluster_labels['charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (cluster_labels['sequence'] + '/' +\n",
    "                                      cluster_labels['charge'].astype(str))\n",
    "        return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS-Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS-Cluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `--mixture-prob <X>`: the probability wrongfully adding a spectrum to a cluster (default X=0.05)\n",
    "- `--num-rounds <X>`: determines how many rounds are used for the hierarchical clustering (default X=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mscluster = {0: 0.0001, 1: 0.001, 2: 0.005, 3: 0.01, 4: 0.05, 5: 0.1}\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_mscluster = os.path.join(cluster_dir, split_i, 'mscluster')\n",
    "    if not os.path.exists(dir_mscluster):\n",
    "        os.makedirs(dir_mscluster)    \n",
    "    # MS-Cluster preprocessing.\n",
    "    cmd = f\"\"\"realpath {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "        > {dir_mscluster}/mscluster_spec_list.txt\"\"\"\n",
    "    ! eval {cmd}\n",
    "    # MS-Cluster clustering.\n",
    "    for i, mixture_prob in hp_mscluster.items():\n",
    "        logger.info('MS-Cluster run %d (mixture-prob=%.4f ; num-rounds=%d)',\n",
    "                    i + 1, mixture_prob, rounds)\n",
    "        dir_mscluster_i = os.path.join(dir_mscluster, f'clusters_{i}')\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"$GLEAMS_HOME/bin/MsCluster/MsCluster \\\n",
    "            --model LTQ_TRYP \\\n",
    "            --list {dir_mscluster}/mscluster_spec_list.txt \\\n",
    "            --output-name mscluster \\\n",
    "            --output-file-size 100000000 \\\n",
    "            --tmp-dir {os.path.join(dir_mscluster, \"tmp\")} \\\n",
    "            --out-dir {dir_mscluster_i} \\\n",
    "            --model-dir $GLEAMS_HOME/bin/MsCluster/Models \\\n",
    "            --memory-gb 300 \\\n",
    "            --fragment-tolerance 0.05 \\\n",
    "            --precursor-ppm 10 \\\n",
    "            --assign-charges \\\n",
    "            --mixture-prob {mixture_prob} \\\n",
    "            --num-rounds {rounds} \\\n",
    "            --keep-dataset-idx\"\"\"\n",
    "        if not os.path.exists(dir_mscluster_i):\n",
    "            ! eval {cmd}\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_mscluster(\n",
    "            os.path.join(dir_mscluster_i, 'clust'), metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'MS-Cluster', (mixture_prob, rounds),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectra-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectra-cluster hyperparameters that influence the clustering quality are:\n",
    "\n",
    "- `-rounds <arg>`: number of clustering rounds to use.\n",
    "- `-threshold_end <arg>`: (lowest) final clustering threshold\n",
    "- `-threshold_start <arg>`: (highest) starting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_spectracluster = {0: 0.99999, 1: 0.9999, 2: 0.999, 3: 0.99, 4: 0.95,\n",
    "                     5: 0.9, 6: 0.8}\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    metadata = pd.read_parquet(os.path.join(cluster_dir, split_i,\n",
    "                                            'cluster_comparison.parquet'))\n",
    "    dir_spectracluster = os.path.join(cluster_dir, split_i, 'spectra-cluster')\n",
    "    if not os.path.exists(os.path.join(dir_spectracluster, 'tmp')):\n",
    "        os.makedirs(os.path.join(dir_spectracluster, 'tmp'))\n",
    "    # spectra-cluster clustering.\n",
    "    for i, threshold_end in hp_spectracluster.items():\n",
    "        logger.info('spectra-cluster run %d (threshold_end=%.4f ; rounds=%d)',\n",
    "                    i + 1, threshold_end, rounds)\n",
    "        filename = os.path.join(dir_spectracluster, f'clusters_{i}.txt')\n",
    "        # Execute clustering.\n",
    "        cmd = f\"\"\"java -jar $GLEAMS_HOME/bin/spectra-cluster/spectra-cluster-cli-1.1.2.jar \\\n",
    "            {os.path.join(cluster_dir, split_i)}/cluster_comparison.mgf \\\n",
    "            -binary_directory {dir_spectracluster}/tmp \\\n",
    "            -fast_mode \\\n",
    "            -fragment_tolerance 0.05 \\\n",
    "            -keep_binary_files \\\n",
    "            -major_peak_jobs $(nproc --all) \\\n",
    "            -output_path {filename} \\\n",
    "            -precursor_tolerance 10 \\\n",
    "            -precursor_tolerance_unit ppm \\\n",
    "            -reuse_binary_files \\\n",
    "            -rounds {rounds} \\\n",
    "            -threshold_end {threshold_end} \\\n",
    "            -threshold_start 1.0 \\\n",
    "            -x_disable_mgf_comments\"\"\"\n",
    "        if not os.path.isfile(filename):\n",
    "            ! eval {cmd}\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_spectracluster(filename, metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'spectra-cluster',\n",
    "                                (threshold_end, rounds),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLEAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLEAMS hyperparameters that influence the clustering quality are ([Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)):\n",
    "\n",
    "- `eps`: The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
    "- `min_samples`:     The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_gleams = {0: 0.02, 1: 0.03, 2: 0.04, 3: 0.05, 4: 0.06, 5: 0.07, 6: 0.08,\n",
    "             7: 0.09, 8: 0.10}\n",
    "min_samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_i in os.listdir(cluster_dir):\n",
    "    dir_gleams = os.path.join(cluster_dir, split_i, 'gleams')\n",
    "    if not os.path.exists(dir_gleams):\n",
    "        os.makedirs(dir_gleams)\n",
    "    metadata_filename = os.path.join(dir_gleams, 'cluster_comparison.parquet')\n",
    "    embed_filename = os.path.join(dir_gleams, 'embed_cluster_comparison.npy')\n",
    "    dist_filename = os.path.join(dir_gleams, 'dist_cluster_comparison.npz')\n",
    "    clust_filename = os.path.join(dir_gleams, 'clusters_cluster_comparison.npy')\n",
    "    shutil.copy(os.path.join(cluster_dir, split_i, 'cluster_comparison.parquet'),\n",
    "                metadata_filename)\n",
    "    metadata = pd.read_parquet(metadata_filename)\n",
    "    # Extract the relevant entries from all (previously computed) embeddings.\n",
    "    if not os.path.isfile(embed_filename):\n",
    "        embed_idx = pd.merge(\n",
    "            metadata, pd.read_parquet(\n",
    "                os.path.join(\n",
    "                    os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "                    f'embed_{config.massivekb_task_id}_{split}.parquet'),\n",
    "                columns=['dataset', 'filename', 'scan']).reset_index(),\n",
    "            'left', ['dataset', 'filename', 'scan'])['index'].values\n",
    "        embeddings = np.load(os.path.join(\n",
    "            os.environ['GLEAMS_HOME'], 'data', 'embed',\n",
    "            f'embed_{config.massivekb_task_id}_{split}.npy'), mmap_mode='r')\n",
    "        np.save(embed_filename, embeddings[embed_idx])\n",
    "    # Compute pairwise distances.\n",
    "    if not os.path.isfile(dist_filename):\n",
    "        cluster.compute_pairwise_distances(\n",
    "            embed_filename, metadata_filename, config.charges)\n",
    "        shutil.move(os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "                                 'dist_cluster_comparison.npz'),\n",
    "                    dist_filename)\n",
    "        shutil.move(os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "                                 os.path.basename(metadata_filename)),\n",
    "                    metadata_filename)\n",
    "        shutil.move(os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "                                 os.path.basename(embed_filename)),\n",
    "                    embed_filename)\n",
    "        metadata = pd.read_parquet(metadata_filename)\n",
    "    # GLEAMS clustering.\n",
    "    for i, eps in hp_gleams.items():\n",
    "        logger.info('GLEAMS run %d (eps=%.3f ; min_samples=%d)',\n",
    "                    i + 1, eps, min_samples)\n",
    "        if os.path.isfile(clust_filename):\n",
    "            os.remove(clust_filename)\n",
    "        config.eps, config.min_samples = eps, min_samples\n",
    "        cluster_filename_i = clust_filename.replace('.npy', f'_{i}.npy')\n",
    "        # Execute clustering.\n",
    "        if not os.path.isfile(cluster_filename_i):\n",
    "            cluster.cluster(dist_filename, metadata_filename)\n",
    "            # Rename file to retain clustering results.\n",
    "            shutil.move(clust_filename, cluster_filename_i)\n",
    "        # Evaluate clustering performance.\n",
    "        cluster_labels = get_clusters_gleams(cluster_filename_i, metadata)\n",
    "        for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "            num_clustered, num_noise, \\\n",
    "                prop_clustered, prop_clustered_incorrect, \\\n",
    "                homogeneity, completeness = \\\n",
    "                    evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                      max_cluster_size)\n",
    "            performance.append((split_i, 'GLEAMS', (eps, min_samples),\n",
    "                                min_cluster_size, max_cluster_size,\n",
    "                                num_clustered, num_noise,\n",
    "                                prop_clustered, prop_clustered_incorrect,\n",
    "                                homogeneity, completeness))\n",
    "    # Clean up ANN indexes.\n",
    "    ann_dir = os.path.join(os.environ['GLEAMS_HOME'], 'data', 'cluster', 'ann')\n",
    "    for filename in os.listdir(ann_dir):\n",
    "        if filename.startswith('ann_cluster_comparison_'):\n",
    "            os.remove(os.path.join(ann_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(performance, columns=[\n",
    "    'split_i', 'tool', 'hyperparameters',\n",
    "    'min_cluster_size', 'max_cluster_size',\n",
    "    'num_clustered', 'num_noise',\n",
    "    'prop_clustered', 'prop_clustered_incorrect',\n",
    "    'homogeneity', 'completeness'])\n",
    "performance.to_csv('cluster_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.read_csv('cluster_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, axes = plt.subplots(1, 2, figsize=(width * 2, height))\n",
    "\n",
    "for tool in ('GLEAMS', 'MS-Cluster', 'spectra-cluster'):\n",
    "    tool_performance = performance[performance['tool'] == tool]\n",
    "    axes[0].errorbar(\n",
    "        x=(tool_performance.groupby('hyperparameters')\n",
    "           ['prop_clustered_incorrect'].mean()),\n",
    "        y=(tool_performance.groupby('hyperparameters')\n",
    "           ['prop_clustered'].mean()),\n",
    "        xerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['prop_clustered_incorrect'].std()),\n",
    "        yerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['prop_clustered'].std()),\n",
    "        marker='o', label=tool)\n",
    "    axes[1].errorbar(\n",
    "        x=(tool_performance.groupby('hyperparameters')\n",
    "           ['prop_clustered_incorrect'].mean()),\n",
    "        y=(tool_performance.groupby('hyperparameters')\n",
    "           ['completeness'].mean()),\n",
    "        xerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['prop_clustered_incorrect'].std()),\n",
    "        yerr=(tool_performance.groupby('hyperparameters')\n",
    "              ['completeness'].std()),\n",
    "        marker='o', label=tool)\n",
    "\n",
    "axes[0].set_xlim(0, 0.05)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[1].set_xlim(0, 0.05)\n",
    "axes[1].set_ylim(0.9, 1)\n",
    "\n",
    "axes[0].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[0].yaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[1].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "\n",
    "axes[0].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[0].set_ylabel('Clustered spectra')\n",
    "axes[1].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[1].set_ylabel('Completeness')\n",
    "\n",
    "axes[0].legend(loc='lower right', frameon=False)\n",
    "axes[1].legend(loc='lower right', frameon=False)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "for ax in axes:\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "plt.savefig('cluster_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
