{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['GLEAMS_HOME'] = os.path.join(os.environ['HOME'],\n",
    "                                         'Projects/gleams')\n",
    "# Make sure all code is in the PATH.\n",
    "sys.path.append(\n",
    "    os.path.normpath(os.path.join(os.environ['GLEAMS_HOME'], 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import EfficiencyWarning\n",
    "warnings.simplefilter(action='ignore', category=EfficiencyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skopt\n",
    "from sklearn.metrics import homogeneity_score, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "from gleams import logger as glogger\n",
    "glogger.init()\n",
    "# Initialize all random seeds before importing any packages.\n",
    "from gleams import rndm\n",
    "rndm.set_seeds()\n",
    "\n",
    "from gleams import config\n",
    "from gleams.cluster import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('gleams')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)    # Single-column figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 2\n",
    "max_prop_clust_incorrect = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filename_ident = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'metadata',\n",
    "    f'massivekb_ids_{config.massivekb_task_id}.parquet')\n",
    "\n",
    "metadata_filename_val = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'embed_{config.massivekb_task_id}_val.parquet')\n",
    "embed_filename_val = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'embed_{config.massivekb_task_id}_val.npy')\n",
    "cluster_filename_val = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'clusters_{config.massivekb_task_id}_val.npy')\n",
    "dist_filename_val = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'dist_{config.massivekb_task_id}_val.npz')\n",
    "\n",
    "metadata_filename_test = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'embed_{config.massivekb_task_id}_test.parquet')\n",
    "embed_filename_test = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'embed_{config.massivekb_task_id}_test.npy')\n",
    "cluster_filename_test = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'clusters_{config.massivekb_task_id}_test.npy')\n",
    "dist_filename_test = os.path.join(\n",
    "    os.environ['GLEAMS_HOME'], 'data', 'cluster',\n",
    "    f'dist_{config.massivekb_task_id}_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the pairwise distances are precomputed.\n",
    "cluster.compute_pairwise_distances(embed_filename_val, metadata_filename_val)\n",
    "cluster.compute_pairwise_distances(embed_filename_test, metadata_filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(metadata_filename, metadata_filename_ident):\n",
    "    metadata = pd.merge(pd.read_parquet(metadata_filename),\n",
    "                        pd.read_parquet(metadata_filename_ident)\n",
    "                        [['dataset', 'filename', 'scan', 'sequence']],\n",
    "                        'left', ['dataset', 'filename', 'scan'], copy=False)\n",
    "    # Don't disambiguate between I/L.\n",
    "    metadata['sequence'] = (metadata['sequence'].str.replace('I', 'L')\n",
    "                            + '/' + metadata['charge'].astype(str))\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata including all spectrum identifications.\n",
    "metadata_val = get_metadata(metadata_filename_val, metadata_filename_ident)\n",
    "metadata_test = get_metadata(metadata_filename_test, metadata_filename_ident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_majority_label_mismatch(labels):\n",
    "    labels_assigned = labels.dropna()\n",
    "    if len(labels_assigned) <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(labels_assigned) - labels_assigned.value_counts().iat[0]\n",
    "\n",
    "\n",
    "def evaluate_clusters(clusters, min_cluster_size=None, max_cluster_size=None):\n",
    "    clusters = clusters.copy()\n",
    "    # Only consider clusters with specific minimum (inclusive) and/or\n",
    "    # maximum (exclusive) size.\n",
    "    cluster_counts = clusters['cluster'].value_counts(dropna=False)\n",
    "    if min_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts < min_cluster_size].index), 'cluster'] = -1\n",
    "    if max_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts >= max_cluster_size].index), 'cluster'] = -1\n",
    "\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = (clusters['cluster'].value_counts(dropna=False)\n",
    "                   .drop(index=-1).to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "    num_clusters = clusters['cluster'].max() + 1\n",
    "\n",
    "    # Reassign noise points to singleton clusters.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_noise = noise_mask.sum()\n",
    "    clusters.loc[noise_mask, 'cluster'] = np.arange(\n",
    "        num_clusters, num_clusters + num_noise)\n",
    "\n",
    "    # Compute cluster evaluation measures.\n",
    "    prop_clustered = (len(clusters) - num_noise) / len(clusters)\n",
    "\n",
    "    clusters_ident = clusters.dropna(subset=['sequence'])\n",
    "    clusters_ident_non_noise = (clusters[~noise_mask]\n",
    "                                .dropna(subset=['sequence']))\n",
    "\n",
    "    # The number of incorrectly clustered spectra is the number of PSMs that\n",
    "    # differ from the majority PSM. Unidentified spectra are not considered.\n",
    "    prop_clustered_incorrect = sum(joblib.Parallel(n_jobs=-1)(\n",
    "        joblib.delayed(_count_majority_label_mismatch)(clust['sequence'])\n",
    "        for _, clust in clusters[~noise_mask].groupby('cluster')))\n",
    "    prop_clustered_incorrect /= len(clusters_ident_non_noise)\n",
    "\n",
    "    # Homogeneity measures whether clusters contain only identical PSMs.\n",
    "    # This is only evaluated on non-noise points, because the noise cluster\n",
    "    # is highly non-homogeneous by definition.\n",
    "    homogeneity = homogeneity_score(clusters_ident_non_noise['sequence'],\n",
    "                                    clusters_ident_non_noise['cluster'])\n",
    "    # Completeness measures whether identical PSMs are assigned to the same\n",
    "    # cluster.\n",
    "    # This is evaluated on all PSMs, including those clustered as noise.\n",
    "    completeness = completeness_score(clusters_ident['sequence'],\n",
    "                                      clusters_ident['cluster'])\n",
    "\n",
    "    return (len(clusters) - num_noise, num_noise,\n",
    "            prop_clustered, prop_clustered_incorrect,\n",
    "            homogeneity, completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster(eps, min_samples, cluster_filename, dist_filename,\n",
    "                metadata_filename, sequences, min_cluster_size):\n",
    "    if os.path.isfile(cluster_filename):\n",
    "        os.remove(cluster_filename)\n",
    "    config.eps, config.min_samples = eps, min_samples\n",
    "    cluster.cluster(dist_filename, metadata_filename)\n",
    "    return evaluate_clusters(\n",
    "        pd.DataFrame({'sequence': sequences,\n",
    "                      'cluster': np.load(cluster_filename)}),\n",
    "        min_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cluster_val = functools.partial(\n",
    "    run_cluster,\n",
    "    cluster_filename=cluster_filename_val,\n",
    "    metadata_filename=metadata_filename_val,\n",
    "    sequences=metadata_val['sequence'],\n",
    "    dist_filename=dist_filename_val,\n",
    "    min_cluster_size=min_cluster_size)\n",
    "\n",
    "\n",
    "def cluster_optim(args):\n",
    "    _, _, prop_clust, prop_clust_incorrect, _, _ = run_cluster_val(*args)\n",
    "    props_clust.append(prop_clust)\n",
    "    props_clust_incorrect.append(prop_clust_incorrect)\n",
    "    if prop_clust_incorrect > max_prop_clust_incorrect:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 - prop_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_clust, props_clust_incorrect = [], []\n",
    "optim = skopt.gp_minimize(cluster_optim,\n",
    "                          [skopt.space.Real(0.01, config.margin / 3, name='eps'),\n",
    "                           skopt.space.Integer(2, 5, name='min_samples')],\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove final (potentially suboptimal) clustering.\n",
    "os.remove(cluster_filename_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_hyperparameter = pd.DataFrame(\n",
    "    {'eps': np.asarray(optim.x_iters)[:, 0],\n",
    "     'min_samples': np.asarray(optim.x_iters)[:, 1],\n",
    "     'prop_clustered': props_clust,\n",
    "     'prop_clustered_incorrect': props_clust_incorrect})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_hyperparam = cluster_hyperparameter[\n",
    "    cluster_hyperparameter['prop_clustered_incorrect']\n",
    "    < max_prop_clust_incorrect]\n",
    "best_hyperparam = acceptable_hyperparam.loc[\n",
    "    acceptable_hyperparam['prop_clustered'].idxmax()]\n",
    "print(f'Optimal clustering hyperparameters:\\n'\n",
    "      f'  - eps = {best_hyperparam[\"eps\"]:.4f}\\n'\n",
    "      f'  - min_samples = {best_hyperparam[\"min_samples\"]:.0f}\\n'\n",
    "      f'-> {best_hyperparam[\"prop_clustered\"]:.2%} clustered, '\n",
    "      f'{best_hyperparam[\"prop_clustered_incorrect\"]:.2%} clustered incorrectly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate clustering hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster on the test set with the optimal hyperparameters.\n",
    "prop_clust_test, prop_clust_incorrect_test, _, _ =  run_cluster(\n",
    "    best_hyperparam['eps'], best_hyperparam['min_samples'],\n",
    "    cluster_filename_test, dist_filename_test, metadata_filename_test,\n",
    "    metadata_test['sequence'], min_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_clust_test, prop_clust_incorrect_test\n",
    "print(f'Cluster the test dataset with optimal hyperparameters\\n'\n",
    "      f'-> {prop_clust_test:.2%} clustered, '\n",
    "      f'{prop_clust_incorrect_test:.2%} clustered incorrectly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(\n",
    "    (cluster_hyperparameter, (prop_clust_test, prop_clust_incorrect_test)),\n",
    "    'cluster_hyperparameter.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_hyperparameter, (prop_clust_test, prop_clust_incorrect_test) = \\\n",
    "#     joblib.load('cluster_hyperparameter.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pareto_frontier(arr):\n",
    "    # Sort by the first column.\n",
    "    arr_sorted = arr[arr[:, 0].argsort()]\n",
    "    # Iteratively add points to the Pareto frontier.\n",
    "    pareto_idx = [0]\n",
    "    for i in range(1, arr_sorted.shape[0]):\n",
    "        if (arr_sorted[i, 0] > arr_sorted[pareto_idx[-1], 0] and\n",
    "                arr_sorted[i, 1] > arr_sorted[pareto_idx[-1], 1]):\n",
    "            pareto_idx.append(i)\n",
    "    return arr_sorted[pareto_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618    # golden ratio\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "# Hyperparameter optimization.\n",
    "clustering_pareto = get_pareto_frontier(np.column_stack(\n",
    "    [cluster_hyperparameter['prop_clustered_incorrect'],\n",
    "     cluster_hyperparameter['prop_clustered']]))\n",
    "ax.plot(clustering_pareto[:, 0], clustering_pareto[:, 1], marker='o',\n",
    "        markersize=7, label='validation')\n",
    "ax.scatter(cluster_hyperparameter['prop_clustered_incorrect'],\n",
    "           cluster_hyperparameter['prop_clustered'], marker='.', s=49)\n",
    "ax.axvline(max_prop_clust_incorrect, c='darkgray', ls='--')\n",
    "\n",
    "# Performance of optimal hyperparameters on validation set.\n",
    "ax.scatter(prop_clust_incorrect_test, prop_clust_test, marker='s', s=49,\n",
    "           label='test', zorder=10)\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "ax.set_xlim(0, 0.05)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel('Incorrectly clustered spectra')\n",
    "ax.set_ylabel('Clustered spectra')\n",
    "\n",
    "ax.xaxis.set_major_formatter(mticker.PercentFormatter(1))\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1))\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('cluster_hyperparameter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
